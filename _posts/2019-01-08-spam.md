---
title: "Building a Spam Classifier from Scratch"
date: 2019-01-08
tags: [nlp-information retrieval-sentiment analysis-Spam Detection]
header:
  image: "/images/spam/bow.png"
  caption: "Photo Credit: freecodecamp.org"
excerpt: "Spam detection for an SMS dataset from Kaggle"
mathjax: "true"
---
### Introduction

Spam detection is a classic exercise that presents a solid introduction to machine learning in practice. In this post, we will use some basic tools of natural language processing for sentiment analysis of an SMS dataset from Kaggle. These techniques should allow us to build a system which captures the semantics of each text message and utilize supervised learning algorithms for performance evaluation.

### Background

Given our set of text messages, the task is to determine whether each message is spam or not. Our data is a combination of overlapping words/characters/symbols from each message. Our system should learn from the patterns and frequencies provided by the data to establish a baseline for predictions. This can be broken down into several stages.

First, we need to convert our messages into an exhaustive vocabulary list. Next, we will utilize some processing techniques to control for fixed-effects such as part of speech, punctuation, capital letters, etc. This should produce a better signal for our model by reducing redundancy and noise from our vocabulary list. Finally, we will convert our list into a vector where each character sequence has a corresponding index value.

If a message contains a given character sequence from our list, the corresponding vector index will contain a value of one and zero otherwise. We will effectively transform each message into a vector of ones and zeros indexed by character sequence. All together, the cumulation of these vectors will represent our feature matrix. We will feed this matrix into our algorithms and evaluate its performance. Let's get started!


### Stage One - Building the List

Alright, let's import our dataset and take care of some cleaning. We'll drop a few unnecessary columns, rename the ones we're keeping, and map 'ham' and 'spam' entries from our target to zeros and ones. Our final preprocessing step is to convert each text message into a list of character sequences, which is also known as tokenizing. All of this work can be done using two python libraries: pandas and nltk. The nltk package provides functionality specific to the natural language processing domain, while pandas is a general platform for data analysis. Let's have a look at a few tokenized messages.

![png](/images/spam/token-text.png?raw=True)

As seen from the image, each message is now a list of character sequences. Here is the small function used for the preprocessing (all code can be found on my [GitHub](https://github.com/l0rdm0rd)):

```python
def process_spam_data(filepath):
  data = pd.read_csv(filepath, encoding="latin-1")
  data = data[['v1','v2']]
  data = data.rename(columns={'v1':'label','v2':'text'})
  data['label'].replace(['ham','spam'],[0,1], inplace=True)
  data['token_text'] = data['text'].apply(nltk.word_tokenize)
  return data
```

### Stage Two - NLTK Processing

Now we are ready to start building our feature matrix where each row will represent a text message from our training set with ones in columns corresponding to the character sequences in the given message (zeros otherwise). Our goal is to capture as much semantic meaning as possible while maintaining a scalable process with efficient use of computational resources. This dataset contains almost 12,000 unique words and not all return the same signal. Training on a feature set of this size would also be very costly, so we will use some processing techniques to trim it down a bit.

To start, the NLTK package provides a list of stop words which are a compilation of the most common words observed in text. Stop words provide a lower signal since they are generally observed at a higher frequency. In other words, their semantic meaning changes very little within context. As a result, these words can be safely removed from our list. By simply removing these common words, our list's length was reduced to about 9800. This is an effective decrease of ~17%. Not bad! Let's have a look at the top ten stop words removed from our vocabulary list:

![png](/images/spam/stop-words.png?raw=True)

It's not too surprising to see some pronouns, prepositions, and conjunctions at the top of the list as these parts of speech provide the lowest value in terms of signal. I also added punctuation to the stop-words list (effectively removing punctuation from the dataset).

Next in the process is normalizing our list of terms to their base form. A word can have many representations depending on its part of speech, so we want to reduce these different word endings to the base form, or lemma. NLTK has a class for lemmatization which requires the word representation and its part of speech. I wrote a function to map the nltk tags to the part of speech parameter for the lemmatization class:

```python
def part_of_speech(tag):
    pos = ""
    if (word[1] == 'VB') | (word[1] == 'VBD') | (word[1] == 'VBG') | (word[1] == 'VBN') | (word[1] == 'VBP') | (word[1] == 'VBZ'):
        pos = 'v'
    elif (word[1] == 'RB') | (word[1] == 'RBR') | (word[1] == 'RBS') | (word[1] == 'WRB'):
        pos = 'r'
    elif (word[1] == 'JJ') | (word[1] == 'JJR') | (word[1] == 'JJS'):
        pos = 'a'
    else:
        pos = 'n'
    return pos
```
This function isn't the most elegant but it gets the job done. From here, we'll use our word list to obtain the part of speech tags, use the above function to map the tags to the parameter value for the lemmatization class and finally normalize our word list.

```python
from nltk.stem import WordNetLemmatizer

words = []
for i in data['token_text']:
    words += i
print(len(set(words)))
words = [w.lower() for w in words if w not in stop_words]
tagged_words = nltk.pos_tag(words)

lemmatizer = WordNetLemmatizer()

def lemmatize_words(tagged_words):
    lemmatized_words = []
    for word in tagged_words:
        lemmatized_words.append(lemmatizer.lemmatize(word[0], pos=part_of_speech(word[1])))
    lemma_counts = Counter(lemmatized_words)
    return lemma_counts
```
The end result is a dictionary containing the base representation of each word and its associated frequency. As a result, the unique set of words was reduced from 9803 to 8852. These basic techniques allowed us to discard ~25% of the low yield content. Pretty sweet! Now what's left is vectorizing our vocabulary list and transforming our dataset into a matrix.

### Stage 3 - Vectorize the Corpus
